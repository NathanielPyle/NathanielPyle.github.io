---
title: "Wine Quality Dataset"
output: 
  html_document:
    code_folding: hide
    fig_width: 7
    fig_height: 6
    fig_captions: true
    theme: darkly
    highlight: default
    toc: False
    number_sections: False
---

## Overview
- **Description**: The quality of wine is determined by various features that relate to its taste, aroma, and other characteristics familiar to wine experts. The UC Irvine Wine Dataset, available at [UC Irvine Wine Dataset](https://archive.ics.uci.edu/dataset/186/wine+quality), c consists of two datasets: one for white wine and one for red wine. These datasets evaluate wine quality using physicochemical tests and sensory variables, focusing on the red and white variants of Portuguese Vinho Verde wine.

The dataset includes 11 features and one dependent variable, the wine quality score. The features, which are based on physicochemical inputs and sensory data, include: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulphur dioxide, total sulphur dioxide, density, pH, sulphates, and alcohol. These measurements were taken based on sensory evaluations, with a median score determined from at least three wine experts' assessments.

Each expert graded the wine samples on a scale from 0 (very bad) to 10 (excellent). Due to privacy and logistical constraints, only physicochemical inputs and sensory outputs are available in this dataset. No information about grape type, wine brand, or other confounding variables is provided.

Notably, the dataset contains no missing data.

The aim of this project was to identify the key variables that influence wine quality. To achieve this, various data science techniques were applied, including correlation matrices, subset selection, model evaluation, stepwise selection, validation and cross-validation using K-folds, as well as Ridge and Lasso regression.

## Methodology

- **Technologies**: the packages that were used in this project were `corrplot`, `readODS`, `ISLR`, `leaps`, and `glmnet`. 


- **Methods**:  The columns in both datasets were assigned names from the source files and the accompanying README. Initially, correlation matrices were created to uncover any basic correlations between variables. To no avail, these correlation matrices provided little information about which variables were the most important, or the nature of the data itself. Therefore, more technical techniques were required to analyse the data itself. 

To overcome the limitations of correlation matrices, subset selection was used to analyse the coefficient of determination(R-Squared) across models as additional variables were incrementally added. Plots were created to show correlation on the y-axis, and the number of variables on the x-axis. 

These plots indicated that as more variables are added, the R-squared value increased, meaning the model explains more variance. However, there is a point of diminishing returns where adding additional variables no longer significantly improves the model's performance. Including all variables in the model would increase accuracy but risk overfitting. Both datasets exhibited this diminishing return effect.

Four plots were created to show the number of variables against Correlation, Adjusted-Correlation, Mallow's Cp, and Bayesian Information Criterion(BIC). An optimal point was identified for each plot, except for the Correlation plot. In addition to these plots, regression subset plots were generated to show which variables were the most important with regards to the Correlation, Adjusted R-squared, Mallow's Cp and BIC. These are similar to bar charts, and show that the higher the bars, the more important the variable. 

Next, Stepwise Selection models—both Forward and Backward—were applied to the White and Red wine datasets. These models aimed to identify the variables that explained the variance in wine quality. Stepwise Selection helps reduce overfitting and eliminate unnecessary variables by iteratively adding and removing variables from the model. Variables not included in the final model were deemed unimportant.  

To validate the findings from the subset selection and Stepwise Selection models, validation and cross-validation were performed. The data for both datasets was split into training and testing partitions. Subset selection was used on the training data to identify the best variables for predicting wine quality, while the testing data was used to construct a model matrix for predictions. 

A loop was implemented to calculate the Mean Squared Error (MSE) for models with varying numbers of predictors from 1 to 11. For each model, coefficients were extracted and used to generate predictions by multiplying them with the appropriate columns of the test model matrix. These predictions were compared with the actual quality values in the test dataset to calculate MSE, which were stored as validation errors. 

The validation errors were analysed to determine which model produced the lowest test MSE. This allowed a comparison between these results, and the previous findings from previous Subset Regression and Stepwise Selection methods. If the number of variables is the same as the previous results, with test MSE in consideration as well, this serves as validation for those findings.

It is important to note that while this approach provides a set of coefficient values for the best model, these values may differ slightly from those produced by Stepwise Selection models. This variation is expected due to the random splitting of the data into training and testing sets. Consequently, while the coefficients will be similar, they may not be identical.

To ease and facilitate predictions, a custom prediction function 'predict.regsubsets' was created.  This function accepts the selected model, constructs a model matrix using new data, and applies the corresponding coefficients to generate predictions. The goal of this function is to enable seamless prediction from the regsubsets output, which is not directly available in base R.

Best subset selection was performed on the full white and red wine datasets, considering models with up to 11 variables. The coefficients for the model with the top 8 variables were extracted, allowing for comparison with previous methods. This step ensures that the full dataset was evaluated for model selection.

To further validate model selection, a k-fold cross-validation process (k = 10) was employed. The dataset was randomly partitioned into 10 folds, with each fold serving as a test set once while the remaining folds formed the training set. Predictions were made for each model size (from 1 to 11 variables) using the predict.regsubsets function, and MSE was calculated for each fold. Cross-validation errors were stored in a matrix.

After the cross-validation process, the average MSE for each model size was calculated. These values were plotted to determine the optimal number of variables based on the lowest cross-validation error. The results indicated that the model with 8 variables for the white wine and 7 for the red produced the lowest mean cross-validation error, validating the previous subset selection findings. The final models, using 8 variables for the white wine dataset and 7 variables for the red wine dataset were confirmed to be the most effective. 

In conclusion, Ridge and Lasso regression were explored as alternative approaches to identify the most important variables. This method involved creating a matrix (x variable) for both the white and red wine datasets using all independent variables, while the response variable (y) was the quality of the wine. A grid of lambda values was defined to test during regularisation. By varying the alpha parameter between 0 (Ridge) and 1 (Lasso), a sequence of lambda values from the grid was applied to regularise the model.

Different lambda values, including 50, 60, and the optimal minimum, were explored, and the corresponding coefficients were analysed. These coefficients were used to make predictions on both datasets. The model was fitted on the training data, and the test MSE was calculated for different lambda values, following a similar split of training and testing data as in Stepwise Selection. An unpenalised least squares model was also used for comparison. Cross-validation determined the optimal lambda value that minimised the test MSE, providing a robust evaluation of the model. Ridge and Lasso regression effectively regularised the variables, with unimportant variables shrunk towards zero.

Lasso regression followed a similar process to Ridge regression, with the key difference being the alpha value set to 1. The same approach was applied to identify the optimal MSE, and through cross-validation, the best set of coefficients that minimised the test MSE was determined. In Lasso regression, many unimportant variables were fully regularised to zero.


## Results

Subset Selection
The results from the white wine dataset were that the model does not significantly improve after the 7th variable. From the first variable to the 8th, the correlation increased from 0.1897253 to 0.2817520. At the 8th variable, the model converges and starts to plateu. 

In comparison the results for the red wine dataset increase rapidly from the 1st variable to the 2nd, from a correlation of 0.2267344 0.3170024. Interestingly, at the 7th variable the model converges and begins to plateu. This difference between the white and red wine datasets indicates that the red wine dataset is more parsimonious than the white wine dataset. These results are crucial for further regression specific algorithms later on in this project.   

The Adjusted R-squared plot shows the variability in the response variable. It is important to note that the model adjusts for the number of predictor variables and penalises multicollinearity. The threshold appears to be around 8 variables, after which any further increase in Adjusted R-squared becomes minimal, indicating that additional variables contribute little to improving the model.

The Mallow's Cp plot strikes a balance between the number of variables and the variance. The lowest Cp value is reached with 8 variables, suggesting this is the optimal number. Beyond this point, additional variables offer little benefit and can be discarded.

The Bayesian Information Criterion (BIC) plot further supports this conclusion. BIC penalises models that are overly complex, favouring simpler, more parsimonious ones. This plot also suggests that a model with 8 variables achieves the most efficient balance between complexity and performance.

For reference, the 8 variables for the white wine dataset are: fixed acidity, volatile acidity, residual sugar, free sulphur dioxide, density, pH, sulphates and alcohol. In comparison, the 7 variables for the red wine datasets are: fixed acidity, volatile acidity, citric acid, chlorides, pH, sulphates and alcohol. As these datasets have some similarity, there is overlap between the same variables. 

When applying this reasoning to the red wine dataset, similar patterns emerge, though with some differences. The Adjusted R-squared still points towards 8 variables, but Mallow's Cp and BIC suggest slightly fewer—7 and 6 variables. This indicates some similarity between the datasets, while also highlighting that the red wine dataset requires fewer variables for an optimal balance between bias, variance, and complexity. Due to this difference, for the white wine dataset 8 variables were used for stepwise selection whereas 7 were used for the red wine dataset; as an average of Mallow's Cp, BIC and the Adjusted R-Squared values. The regression subset plots show which variables are the most important in both the white and red wine datasets. It should be noted that these results are synonymous with the previous subset plots, they just indicate which variables are the most important features.   

As the results for both the White and Red wine datasets were the same for both Forward and Backwards Stepwise Selection, this indicates that both models are stable. Using 8 variables for the White wine and 7 variables for the Red wine datasets means that these number of variables are the most significant features to each model's performance. These results are consistent with the Adjusted R-Squared values, Mallow's Cp and BIC from stepwise selection. In addition to this, the values having the same values for both Forward and Backwards Stepwise Selection indicates that there are no confounding multicollinearity issues. In the beginning of this project, correlation matrices were created to show the relationships variables had with both itself and others. As the results are the same for both Stepwise Selection methods, this indicates that the order of the variables were added or subtracted from the model did not influence the model. 


Using the for loop-6 for the red wine

Using the loop to select the model with the lowest test MSE, for both the white and red wine datasets, there is some variation. For the white wine dataset, the for loop selects a model with 8 variables, which is consistent with the previous results, however for the red wine dataset the for loop selects a model with 6 variables. This score of 6 variables is consistent with the BIC number of variables from Stepwise Selection. Despite being a slight difference of 1 variable, this is not a huge difference and does provide both reliability to the previous findings. It is important to note that 8 and 6 variables were selected as they produce the lowest number of validation errors out of all 11 possible combinations. In addition to this, ensuring that the set.seed function is used is crucial as without this function, the results are inconsistent.  




## Visualisations

## Citations 
As requested by the authors of this dataset, they requested this citation be used. 

P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. 
  Modeling wine preferences by data mining from physicochemical properties.
  In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.

## Further links
For further links please access the following url. 

[Modeling wine preferences by data mining from physicochemical properties](http://dx.doi.org/10.1016/j.dss.2009.05.016)
                
                


