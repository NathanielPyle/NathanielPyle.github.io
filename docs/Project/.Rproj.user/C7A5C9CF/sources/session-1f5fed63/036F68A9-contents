library(corrplot)
library(readODS)
library(ISLR)
library(leaps)
library(glmnet)
set.seed(123)

white_wine <- read.csv("D:/Downloads/winequality-white.csv", header = TRUE, sep = ";")
red_wine <- read.csv("D:/Downloads/winequality-red.csv", header = TRUE, sep = ";")

# column names as shown from the website https://archive.ics.uci.edu/dataset/186/wine+quality
column_names <- c("fixed_acidity", "volatile_acidity", "citric_acid", "residual_sugar", 
                  "chlorides", "free_sulfur_dioxide", "total_sulfur_dioxide", "density", 
                  "pH", "sulphates", "alcohol", "quality")


#basic correlation matrices to understand the relationship between variables
# it is apparent that these do not provide much information that the nature of the data itself
# this is likely due to the h
corrplot(cor(white_wine), order='alphabet')
corrplot(cor(red_wine), order='alphabet')


#dimensions of the red and white wine data sets
dim(white_wine)
dim(red_wine)

# white wine code to understand correlation between the number of variables
regfit.full_white=regsubsets(quality~.,data = white_wine, nvmax = 12)
white_reg.summary <- summary(regfit.full_white)
names(white_reg.summary)
white_reg.summary$rsq

# red wine code to understand correlation between the number of variables
regfit.full_red=regsubsets(quality~.,data = red_wine, nvmax = 12)
red_reg.summary <- summary(regfit.full_red)
names(red_reg.summary)
red_reg.summary$rsq

#plotting the number of variables for the red wine versus Adjusted RSQ, CP, and BIC
par(mfrow=c(2,2))
plot(red_reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(red_reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted Rsq", type = "l")
which.max(red_reg.summary$adjr2)
points (8, red_reg.summary$adjr2[8], col ="red",cex =2, pch =20)
plot(red_reg.summary$cp ,xlab =" Number of Variables ",ylab="Cp",type="l")
which.min((red_reg.summary$cp))
points (7, red_reg.summary$cp [7], col ="red",cex =2, pch =20)
which.min(red_reg.summary$bic)
plot(red_reg.summary$bic ,xlab=" Number of Variables ",ylab=" BIC", type="l")
points(6, red_reg.summary$bic [6], col="red",cex = 2, pch =20)


#plotting the number of variables for the white wine versus Adjusted RSQ, CP, and BIC
par(mfrow=c(2,2))
plot(white_reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(white_reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted Rsq", type = "l")
which.max(white_reg.summary$adjr2)
points (8, white_reg.summary$adjr2[8], col ="red",cex =2, pch =20)
plot(white_reg.summary$cp ,xlab =" Number of Variables ",ylab="Cp",type="l")
which.min(white_reg.summary$cp)
points (8, white_reg.summary$cp [8], col ="red",cex =2, pch =20)
which.min(white_reg.summary$bic)
plot(white_reg.summary$bic ,xlab=" Number of Variables ",ylab=" BIC", type="l")
points (8, white_reg.summary$bic [8], col =" red",cex =2, pch =20)

#white wine display of selected variables for the best model
plot(regfit.full_white, scale="r2")
plot(regfit.full_white, scale="adjr2")
plot(regfit.full_white, scale="Cp")
plot(regfit.full_white, scale="bic")



#red wine display of selected variables for the best model
plot(regfit.full_red, scale="r2")
plot(regfit.full_red, scale="adjr2")
plot(regfit.full_red, scale="Cp")
plot(regfit.full_red, scale="bic")


# coef estimates associated with the white wine model
coef(regfit.full_white,4)


# white wine model
regfit.fwd_white = regsubsets(quality ~ ., data = white_wine, nvmax = 11, method = "forward")
summary(regfit.fwd_white)
regfit.bwd_white = regsubsets(quality ~ ., data = white_wine, nvmax = 11, method = "backward")
summary(regfit.bwd_white)

coef(regfit.fwd_white,4)
coef(regfit.bwd_white,4)

train_white = sample(c(TRUE, FALSE), nrow(white_wine),rep=TRUE)
test_white=(!train_white)

regfit.best_white=regsubsets(quality~.,data = white_wine[train_white,],
                       nvmax=11)
test.mat_white =  model.matrix(quality~.,data = white_wine[test_white,])

val.errors = rep(NA,11)
for(i in 1:11){
coefi=coef(regfit.best_white ,id=i)
pred=test.mat_white [,names(coefi)]%*% coefi
val.errors [i]= mean(( white_wine$quality[test_white]-pred)^2)
}

white_val = val.errors
which.min(white_val)
coef(regfit.best_white,6)

predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars] %*% coefi
}



#performing the best subset selection on the full white wine data set
# to select the best model with the top 8 variables
regfit.best_white = regsubsets(quality ~ ., data= white_wine, nvmax = 11)

coef(regfit.best_white, 8)

k = 10
folds <- sample(1:k, nrow(white_wine), replace = TRUE)
cv.errors <- matrix(NA, k, 11, dimnames = list(NULL, paste(1:11)))

for (j in 1:k) {
  best.fit <- regsubsets(quality ~ ., data = white_wine[folds != j,], nvmax = 11)
  for(i in 1:11) {
    pred = predict.regsubsets(best.fit, white_wine[folds == j,], id = i)
    cv.errors[j, i] = mean((white_wine$quality[folds == j] - pred)^2)
  }
}

mean.csv.errors = apply(cv.errors,2,mean)
mean.csv.errors
par(mfrow=c(1,1))
plot(mean.csv.errors, type = 'b')

regfit.best_white = regsubsets(quality~., data = white_wine, nvmax= 8)
coef(regfit.best_white,8)


#ridge regression

x <- model.matrix(quality ~ ., white_wine)[, -1] 
y <- white_wine$quality  
grid = 10^ seq (10,-2, length = 100)


# Fit Ridge regression model
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)

# Dimensions of coefficients matrix
dim(coef(ridge.mod))  # Should output [12 100]

# Coefficients for the 50th lambda value
ridge.mod$lambda[50]  # Corresponds to lambda value
coef(ridge.mod)[, 50]  # Coefficients for the 50th lambda value

# Calculate L2 norm (Euclidean norm) of coefficients at lambda[50] (excluding intercept)
sqrt(sum(coef(ridge.mod)[-1, 50]^2))  # L2 norm

# Coefficients for the 60th lambda value
ridge.mod$lambda[60]  # Corresponds to lambda value
coef(ridge.mod)[, 60]  # Coefficients for the 60th lambda value

# Calculate L2 norm of coefficients at lambda[60] (excluding intercept)
sqrt(sum(coef(ridge.mod)[-1, 60]^2))  # L2 norm

# Use predict with lambda value corresponding to s = 50th lambda
predicted_coefficients <- predict(ridge.mod, s = 50, type = "coefficients")[1:11, ]
predicted_coefficients


train=sample (1: nrow(x), nrow(x)/2)
test=(- train )
y.test=y[test]

ridge.mod = glmnet(x[train,], y[train], alpha = 0, lambda = grid,
                   thresh = 1e-12)
ridge.pred = predict(ridge.mod, s=4, newx = x[test,])
mean((ridge.pred -y.test)^2)

mean((mean(y[train])-y.test)^2)

ridge.pred = predict(ridge.mod, s = 1e10, newx = x[test,])
mean((ridge.pred - y.test)^2)

ridge.pred = predict(ridge.mod, s = 0, newx = x[test,], exact = TRUE, x = x, y = y)
mean((ridge.pred - y.test)^2)

lm(y~x, subset = train)
predict(ridge.mod, s= 0, exact = T,x = x, y = y, type = "coefficients")[1:11,]


cv.out = cv.glmnet(x[train,],y[train], alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam


ridge.pred = predict(ridge.mod, s = bestlam, newx = x[test,])
mean((ridge.pred - y.test)^2)

out = glmnet(x,y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:11,]


lasso.mod = glmnet(x[train,],y[train], alpha = 1, lambda = grid)
plot(lasso.mod)

cv.out = glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
lasso.pred = predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred -y.test)^2)


out = glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef = predict(out, type = "coefficients", s = bestlam)[1:12,]
lasso.coef

lasso.coef[lasso.coef != 0]





# coef estimates associated with the red wine model
coef(regfit.full_red,3)

#red wine
regfit.fwd_red = regsubsets(quality ~ ., data = red_wine, nvmax = 11, method = "forward")
summary(regfit.fwd_red)
regfit.bwd_red = regsubsets(quality ~ ., data = red_wine, nvmax = 11, method = "backward")
summary(regfit.bwd_red)

coef(regfit.fwd_red,3)
coef(regfit.bwd_red,3)

train_red = sample(c(TRUE, FALSE), nrow(red_wine),rep=TRUE)
test_red=(!train_red)

regfit.best_red=regsubsets(quality~.,data = red_wine[train_red,],
                       nvmax=11)
test.mat_red =  model.matrix(quality~.,data = red_wine[test_red,])

val.errors = rep(NA,11)
for(i in 1:11){
  coefi=coef(regfit.best_red ,id=i)
  pred=test.mat_red [,names(coefi)]%*% coefi
  val.errors [i]= mean(( red_wine$quality[test_red]-pred)^2)
}


red_val = val.errors
which.min(red_val)
coef(regfit.best_red,6)



#performing the best subset selection on the full red wine data set
# to select the best model with the top 8 variables
regfit.best_red = regsubsets(quality ~ ., data= red_wine, nvmax=11)

coef(regfit.best_red, 6)

k = 10
folds <- sample(1:k, nrow(red_wine), replace = TRUE)
cv.errors <- matrix(NA, k, 11, dimnames = list(NULL, paste(1:11)))

for (j in 1:k) {
  best.fit <- regsubsets(quality ~ ., data = red_wine[folds != j,], nvmax = 11)
  for(i in 1:11) {
    pred = predict.regsubsets(best.fit, red_wine[folds == j,], id = i)
    cv.errors[j, i] = mean((red_wine$quality[folds == j] - pred)^2)
  }
}

mean.csv.errors = apply(cv.errors,2,mean)
mean.csv.errors
par(mfrow=c(1,1))
plot(mean.csv.errors, type = 'b')

regfit.best_red = regsubsets(quality~., data = red_wine, nvmax= 6)
coef(regfit.best_white,6)

#ridge and lasso regression

x <- model.matrix(quality~., red_wine)[,-1]
y <- red_wine$quality 

grid = 10^seq(10, -2, length = 100)
ridge.mod = glmnet(x,y, alpha = 0, lambda = grid)
dim(coef(ridge.mod))

ridge.mod$lambda [50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))

ridge.mod$lambda[60]

coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))

predict(ridge.mod, s=50, type = "coefficients")[1:12,]

train = sample(1:nrow(x), nrow(x)/2)
test = (-train)
y.test = y[test]


ridge.mod = glmnet(x[train,], y[train], alpha = 0, lambda = grid,
                   thresh = 1e-12)
ridge.pred = predict(ridge.mod, s=4, newx = x[test,])
mean((ridge.pred -y.test)^2)

mean((mean(y[train])-y.test)^2)

ridge.pred = predict(ridge.mod, s = 1e10, newx = x[test,])
mean((ridge.pred - y.test)^2)

ridge.pred = predict(ridge.mod, s = 0, newx = x[test,], exact = TRUE, x = x, y = y)
mean((ridge.pred - y.test)^2)

lm(y~x, subset = train)
predict(ridge.mod, s= 0, exact = T,x = x, y = y, type = "coefficients")[1:11,]


cv.out = cv.glmnet(x[train,],y[train], alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam


ridge.pred = predict(ridge.mod, s = bestlam, newx = x[test,])
mean((ridge.pred - y.test)^2)

#plot of best coefficients using L1 Norm
out = glmnet(x,y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:11,]

#lasso regression

lasso.mod = glmnet(x[train,],y[train], alpha = 1, lambda = grid)
plot(lasso.mod)

cv.out = glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
lasso.pred = predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred -y.test)^2)


out = glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef = predict(out, type = "coefficients", s = bestlam)[1:12,]
lasso.coef

lasso.coef[lasso.coef != 0]
