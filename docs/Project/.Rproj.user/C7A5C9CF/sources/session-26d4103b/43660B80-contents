library(corrplot)
library(readODS)
library(ISLR)
library(leaps)
library(glmnet)
set.seed(123)

white_wine <- read.csv("D:/Downloads/winequality-white.csv", header = TRUE, sep = ";")
red_wine <- read.csv("D:/Downloads/winequality-red.csv", header = TRUE, sep = ";")

# column names as shown from the website https://archive.ics.uci.edu/dataset/186/wine+quality
column_names <- c("fixed_acidity", "volatile_acidity", "citric_acid", "residual_sugar", 
                  "chlorides", "free_sulfur_dioxide", "total_sulfur_dioxide", "density", 
                  "pH", "sulphates", "alcohol", "quality")


#basic correlation matrices to understand the relationship between variables
# it is apparent that these do not provide much information that the nature of the data itself
# this is likely due to multicollinearity between the same variables
corrplot(cor(white_wine), order='alphabet')
corrplot(cor(red_wine), order='alphabet')


#dimensions of the red and white wine data sets
dim(white_wine)
dim(red_wine)

# White wine code to understand the relationship between the number of variables
# and the R-squared values, which represent the proportion of variance explained by the model.
# Analysing the R-squared values helps us assess how well the model explains the variance.
regfit.full_white = regsubsets(quality ~ ., data = white_wine, nvmax = 11)
white_reg.summary <- summary(regfit.full_white)
names(white_reg.summary)

# Plotting R-squared values for models with different numbers of variables (White Wine)
white_reg.summary$rsq
plot(white_reg.summary$rsq, type = "o", col = "blue",
     pch = 16, xlab = "Number of Variables", ylab = "R-squared",
     main = "R-Squared for Different Models (White Wine)",
     lwd = 3)
grid()

# Red wine code to understand the relationship between the number of variables
# and the R-squared values, which represent the proportion of variance explained by the model.
regfit.full_red = regsubsets(quality ~ ., data = red_wine, nvmax = 11)
red_reg.summary <- summary(regfit.full_red)
names(red_reg.summary)
red_reg.summary$rsq

# Plotting R-squared values for models with different numbers of variables (Red Wine)
plot(red_reg.summary$rsq, type = "o", col = "red",
     pch = 16, xlab = "Number of Variables", ylab = "R-squared",
     main = "R-Squared for Different Models (Red Wine)",
     lwd = 3)
grid()

# The residual sum of squares (RSS) plot shows that total error decreases as the number
# of variables increases in the model, leading to a downward slope.
# However, it's important to note that increasing the number of variables only minimally
# decreases the total error after a certain point. Therefore, after a threshold, adding more
# variables may result in overfitting.
par(mfrow = c(2, 2))
plot(white_reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")

# The Adjusted R-squared plot shows the variability explained in the response variable.
# It is important to note that the model adjusts for the number of predictor variables
# and penalises unnecessary complexity. The threshold for this is around 8 variables,
# after which the increase becomes negligible.
plot(white_reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted Rsq", type = "l")
which.max(white_reg.summary$adjr2)
points(8, white_reg.summary$adjr2[8], col = "blue", cex = 2, pch = 20)

# The Mallow's Cp plot compares models by balancing the total number of variables with
# variance. The minimum Cp value occurs at 8 variables, suggesting that models with 8 variables
# should be used, while the other variables add little value and can be discarded.
plot(white_reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(white_reg.summary$cp)
points(8, white_reg.summary$cp[8], col = "blue", cex = 2, pch = 20)

# The Bayesian Information Criterion (BIC) plot is a criterion for model selection.
# Models that are too complex are penalised, and more parsimonious models are prioritised.
# This indicates that a model with 8 variables is the most parsimonious.
which.min(white_reg.summary$bic)
plot(white_reg.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
points(8, white_reg.summary$bic[8], col = "blue", cex = 2, pch = 20)

# Using the previous reasoning for the white wine dataset, the following models
# had similar results. The total number of variables according to the Adjusted R-squared
# value was 8, but the red wine dataset showed different values for the Mallow's Cp
# and BIC plots, which were 7 and 6 respectively. 
# This indicates some similarity between the two datasets, but the red wine dataset
# prioritises fewer variables for an optimal balance between bias, variance, and complexity.
par(mfrow = c(2, 2))
plot(red_reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(red_reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted Rsq", type = "l")
which.max(red_reg.summary$adjr2)
points(8, red_reg.summary$adjr2[8], col = "red", cex = 2, pch = 20)
plot(red_reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(red_reg.summary$cp)
points(7, red_reg.summary$cp[7], col = "red", cex = 2, pch = 20)
which.min(red_reg.summary$bic)
plot(red_reg.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
points(6, red_reg.summary$bic[6], col = "red", cex = 2, pch = 20)

#white wine subset selection plots 
plot(regfit.full_white, scale="r2")
plot(regfit.full_white, scale="adjr2")
plot(regfit.full_white, scale="Cp")
plot(regfit.full_white, scale="bic")

#red wine subset selection plots 
plot(regfit.full_red, scale="r2")
plot(regfit.full_red, scale="adjr2")
plot(regfit.full_red, scale="Cp")
plot(regfit.full_red, scale="bic")


# coef estimates associated with the white wine model
# using the previous results as a basis, there are 8 variables which are the most
# important to the performance of the white wine model
coef(regfit.full_white,8)


# white wine model forward and backward stepwise selection 
regfit.fwd_white = regsubsets(quality ~ ., data = white_wine, nvmax = 11, method = "forward")
summary(regfit.fwd_white)
regfit.bwd_white = regsubsets(quality ~ ., data = white_wine, nvmax = 11, method = "backward")
summary(regfit.bwd_white)

# interestingly both the forward and backwards stepwise selection use the same 
# variables and have the same values
coef(regfit.fwd_white,8)
coef(regfit.bwd_white,8)


#using the validation and cross-validation approach to select an appropriate model
# for the white wine data set

#partition of the training and testing data
set.seed(123)
train_white = sample(c(TRUE, FALSE), nrow(white_wine),rep=TRUE)
test_white=(!train_white)

#best subset selection for the white wine dataset
regfit.best_white=regsubsets(quality~.,data = white_wine[train_white,],
                       nvmax=11)
test.mat_white =  model.matrix(quality~.,data = white_wine[test_white,])


# loop to calculate the best coefficients for regfit.best_white dataset. 
#these coefficients are multiplied by the appropriate columns of the test model 
# matrix to form predictions and calculate the test MSE
val.errors = rep(NA,11)
for(i in 1:11){
coefi=coef(regfit.best_white ,id=i)
pred=test.mat_white [,names(coefi)]%*% coefi
val.errors [i]= mean(( white_wine$quality[test_white]-pred)^2)
}

#the best model contains at most 8 variables. this is the same as the results 
#from before

white_val = val.errors
which.min(white_val)
coef(regfit.best_white,8)

#predict function to validate the previous results
predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars] %*% coefi
}

#performing the best subset selection on the full white wine data set
# to select the best model with the top 8 variables
regfit.best_white = regsubsets(quality ~ ., data= white_wine, nvmax = 11)

#these results are the same as the loop function 
coef(regfit.best_white, 8)


#cross validation using k-folds training sets
k = 10
folds <- sample(1:k, nrow(white_wine), replace = TRUE)
cv.errors <- matrix(NA, k, 11, dimnames = list(NULL, paste(1:11)))


# k-folds specified loop using the predict method. This will also calculate 
# the test errors on the appropriate subset and store them in the matrix cv.errors
for (j in 1:k) {
  best.fit <- regsubsets(quality ~ ., data = white_wine[folds != j,], nvmax = 11)
  for(i in 1:11) {
    pred = predict.regsubsets(best.fit, white_wine[folds == j,], id = i)
    cv.errors[j, i] = mean((white_wine$quality[folds == j] - pred)^2)
  }
}

#plotting the mean.csv.errors against the number of variables, the lowest point
# is at 8, which indicates that the cross validation model which uses 8 variables
# produces the lowest mean.csv.error.
mean.csv.errors = apply(cv.errors,2,mean)
mean.csv.errors
par(mfrow=c(1,1))
plot(mean.csv.errors, type = 'b')

regfit.best_white = regsubsets(quality~., data = white_wine, nvmax= 8)
coef(regfit.best_white,8)

#ridge regression using the last variable, quality, as the dependent or response 
# variable

x <- model.matrix(quality ~ ., white_wine)[, -1] 
y <- white_wine$quality  
grid = 10^ seq (10,-2, length = 100)


# Fit Ridge regression model
# It is important to note that the alpha value is set to 0, indicating this is 
# Ridge regression
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)

# Dimensions of the coefficients matrix
dim(coef(ridge.mod))  

# Coefficients for the 50th lambda value
ridge.mod$lambda[50]  # Corresponds to the 50th lambda value
coef(ridge.mod)[, 50]  # Coefficients at the 50th lambda value

# Calculate the L2 norm (Euclidean norm) of coefficients at lambda[50] (excluding the intercept)
# As these coefficients have been significantly shrunk, L2 regularisation has had a strong effect
sqrt(sum(coef(ridge.mod)[-1, 50]^2))  # L2 norm

# Coefficients for the 60th lambda value
ridge.mod$lambda[60]  # Corresponds to the 60th lambda value
coef(ridge.mod)[, 60]  # Coefficients at the 60th lambda value

# Calculate the L2 norm of coefficients at lambda[60] (excluding the intercept)
# As the results are larger compared to those at lambda[50], this indicates
# that L2 regularisation has penalised the coefficients less 
sqrt(sum(coef(ridge.mod)[-1, 60]^2))  # L2 norm

# Use predict with lambda value corresponding to s = 50th lambda
# Using the results where lambda = 50, generated smaller coefficients that were
# less prone to overfitting. 
predicted_coefficients <- predict(ridge.mod, s = 50, type = "coefficients")[1:11, ]
predicted_coefficients

# Train-test split
train = sample(1:nrow(x), nrow(x) / 2)
test = (-train)
y.test = y[test]

# Fit a Ridge regression model on the training set to evaluate the test MSE
# Using a large lambda value of 4
ridge.mod = glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred = predict(ridge.mod, s = 4, newx = x[test,])
mean((ridge.pred - y.test)^2)

# Method to calculate the test MSE using intercepts
mean((mean(y[train]) - y.test)^2)

# Fit a Ridge regression model with an extremely large lambda (1e10) and calculate the test MSE
ridge.pred = predict(ridge.mod, s = 1e10, newx = x[test,])
mean((ridge.pred - y.test)^2)

# Another method to calculate test MSE with exact regularisation and training data considered
ridge.pred = predict(ridge.mod, s = 1e10, newx = x[test,], exact = TRUE, x = x, y = y)
mean((ridge.pred - y.test)^2)

# Fitting a Ridge regression model where lambda = 4 leads to a much lower test MSE 
# than a model with just an intercept

# Fit a Ridge regression model without regularisation (s = 0) and calculate the test MSE
ridge.pred = predict(ridge.mod, s = 0, newx = x[test,], exact = TRUE, x = x, y = y)
mean((ridge.pred - y.test)^2)

# Fit an unpenalised least squares model for comparison
lm(y ~ x, subset = train)
predict(ridge.mod, s = 0, exact = TRUE, x = x, y = y, type = "coefficients")[1:11,]

# This indicates that a wide range of lambda values can be used.
# Very small lambda values (e.g., log(lambda) < -2) result in minimal regularisation,
# while larger lambda values penalise large coefficients.
# The optimal lambda lies between these extremes, minimising the test MSE 
# while shrinking the coefficients to balance model complexity.

# Cross-validation to determine the optimal lambda value
cv.out = cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)

# The best lambda value that results in the smallest MSE is determined by cross-validation
bestlam = cv.out$lambda.min
bestlam

# Calculate the optimal test MSE associated with the best lambda value
ridge.pred = predict(ridge.mod, s = bestlam, newx = x[test,])
mean((ridge.pred - y.test)^2)

# Refit the Ridge regression model on the full dataset using the optimal lambda 
# and examine the coefficient estimates
out = glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:11,]


# Fit Lasso regression model with cross-validation to determine the best lambda
cv.lasso <- cv.glmnet(x[train,], y[train], alpha = 1)

# Plot the cross-validation results
plot(cv.lasso)

# Extract the best lambda from cross-validation
bestlam <- cv.lasso$lambda.min

# Use the best lambda to predict on the test data
lasso.pred <- predict(cv.lasso, s = bestlam, newx = x[test,])

# Calculate the test MSE
mean((lasso.pred - y.test)^2)

# Refit the Lasso model on the entire dataset using the best lambda
final.lasso <- glmnet(x, y, alpha = 1, lambda = bestlam)

# Extract the final coefficients
lasso.coef <- predict(final.lasso, type = "coefficients")[1:11, ]
print(lasso.coef)

# Print non-zero coefficients
lasso.coef[lasso.coef != 0]


# coef estimates associated with the red wine model
# using the previous results as a basis, there are 7 variables which are the most
# important to the performance of the red wine model
regfit.best_red = regsubsets(quality~., data = red_wine, nvmax= 7)
coef(regfit.best_red,7)

#red wine forward and backward stepwise 
regfit.fwd_red = regsubsets(quality ~ ., data = red_wine, nvmax = 11, method = "forward")
summary(regfit.fwd_red)
regfit.bwd_red = regsubsets(quality ~ ., data = red_wine, nvmax = 11, method = "backward")
summary(regfit.bwd_red)


#the same coefficient values for both forward and backwards stepwise
coef(regfit.fwd_red,7)
coef(regfit.bwd_red,7)


#partition of the training and testing data
set.seed(123)
train_red = sample(c(TRUE, FALSE), nrow(red_wine),rep=TRUE)
test_red=(!train_red)

regfit.best_red=regsubsets(quality~.,data = red_wine[train_red,],
                       nvmax=11)
test.mat_red =  model.matrix(quality~.,data = red_wine[test_red,])

val.errors = rep(NA,11)
for(i in 1:11){
  coefi=coef(regfit.best_red ,id=i)
  pred=test.mat_red [,names(coefi)]%*% coefi
  val.errors [i]= mean(( red_wine$quality[test_red]-pred)^2)
}


red_val = val.errors
which.min(red_val)
coef(regfit.best_red,6)


#performing the best subset selection on the full red wine data set
# to select the best model with the top 8 variables
regfit.best_red = regsubsets(quality ~ ., data= red_wine, nvmax=11)

coef(regfit.best_red, 6)

k = 10
folds <- sample(1:k, nrow(red_wine), replace = TRUE)
cv.errors <- matrix(NA, k, 11, dimnames = list(NULL, paste(1:11)))

for (j in 1:k) {
  best.fit <- regsubsets(quality ~ ., data = red_wine[folds != j,], nvmax = 11)
  for(i in 1:11) {
    pred = predict.regsubsets(best.fit, red_wine[folds == j,], id = i)
    cv.errors[j, i] = mean((red_wine$quality[folds == j] - pred)^2)
  }
}

mean.csv.errors = apply(cv.errors,2,mean)
mean.csv.errors
par(mfrow=c(1,1))
plot(mean.csv.errors, type = 'b')

regfit.best_red = regsubsets(quality~., data = red_wine, nvmax= 7)
coef(regfit.best_red,7)

#ridge and lasso regression

x <- model.matrix(quality~., red_wine)[,-1]
y <- red_wine$quality 

grid = 10^seq(10, -2, length = 100)

# Fit Ridge regression model
# It is important to note that the alpha value is set to 0, indicating this is 
# Ridge regression
ridge.mod = glmnet(x,y, alpha = 0, lambda = grid)

# Dimensions of the coefficients matrix
dim(coef(ridge.mod))

# Coefficients for the 50th lambda value
ridge.mod$lambda[50]  # Corresponds to the 50th lambda value
coef(ridge.mod)[, 50]  # Coefficients at the 50th lambda value

# Calculate the L2 norm (Euclidean norm) of coefficients at lambda[50] (excluding the intercept)
# As these coefficients have been significantly shrunk, L2 regularisation has had a strong effect
sqrt(sum(coef(ridge.mod)[-1, 50]^2))  # L2 norm

# Coefficients for the 60th lambda value
ridge.mod$lambda[60]  # Corresponds to the 60th lambda value
coef(ridge.mod)[, 60]  # Coefficients at the 60th lambda value

# Calculate the L2 norm of coefficients at lambda[60] (excluding the intercept)
# As the results are larger compared to those at lambda[50], this indicates
# that L2 regularisation has penalised the coefficients less 
sqrt(sum(coef(ridge.mod)[-1, 60]^2))  # L2 norm

# Use predict with lambda value corresponding to s = 50th lambda
# Using the results where lambda = 50, generated smaller coefficients that were
# less prone to overfitting. 
predicted_coefficients <- predict(ridge.mod, s = 50, type = "coefficients")[1:11, ]
predicted_coefficients

# Train-test split
train = sample(1:nrow(x), nrow(x) / 2)
test = (-train)
y.test = y[test]

# Fit a Ridge regression model on the training set to evaluate the test MSE
# Using a large lambda value of 4
ridge.mod = glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred = predict(ridge.mod, s = 4, newx = x[test,])
mean((ridge.pred - y.test)^2)

# Method to calculate the test MSE using intercepts
mean((mean(y[train]) - y.test)^2)

# Fit a Ridge regression model with an extremely large lambda (1e10) and calculate the test MSE
ridge.pred = predict(ridge.mod, s = 1e10, newx = x[test,])
mean((ridge.pred - y.test)^2)

# Another method to calculate test MSE with exact regularisation and training data considered
ridge.pred = predict(ridge.mod, s = 1e10, newx = x[test,], exact = TRUE, x = x, y = y)
mean((ridge.pred - y.test)^2)

# Fitting a Ridge regression model where lambda = 4 leads to a much lower test MSE 
# than a model with just an intercept

# Fit a Ridge regression model without regularisation (s = 0) and calculate the test MSE
ridge.pred = predict(ridge.mod, s = 0, newx = x[test,], exact = TRUE, x = x, y = y)
mean((ridge.pred - y.test)^2)

# Fit an unpenalised least squares model for comparison
lm(y ~ x, subset = train)
predict(ridge.mod, s = 0, exact = TRUE, x = x, y = y, type = "coefficients")[1:11,]

# This indicates that a wide range of lambda values can be used.
# Very small lambda values (e.g., log(lambda) < -2) result in minimal regularisation,
# while larger lambda values penalise large coefficients.
# The optimal lambda lies between these extremes, minimising the test MSE 
# while shrinking the coefficients to balance model complexity.

# Cross-validation to determine the optimal lambda value
cv.out = cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)

# The best lambda value that results in the smallest MSE is determined by cross-validation
bestlam = cv.out$lambda.min
bestlam

# Calculate the optimal test MSE associated with the best lambda value
ridge.pred = predict(ridge.mod, s = bestlam, newx = x[test,])
mean((ridge.pred - y.test)^2)

# Refit the Ridge regression model on the full dataset using the optimal lambda 
# and examine the coefficient estimates
out = glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:11,]


# Fit Lasso regression model with cross-validation to determine the best lambda
cv.lasso <- cv.glmnet(x[train,], y[train], alpha = 1)

# Plot the cross-validation results
plot(cv.lasso)

# Extract the best lambda from cross-validation
bestlam <- cv.lasso$lambda.min

# Use the best lambda to predict on the test data
lasso.pred <- predict(cv.lasso, s = bestlam, newx = x[test,])

# Calculate the test MSE
mean((lasso.pred - y.test)^2)

# Refit the Lasso model on the entire dataset using the best lambda
final.lasso <- glmnet(x, y, alpha = 1, lambda = bestlam)

# Extract the final coefficients
lasso.coef <- predict(final.lasso, type = "coefficients")[1:11, ]
print(lasso.coef)

# Print non-zero coefficients
lasso.coef[lasso.coef != 0]