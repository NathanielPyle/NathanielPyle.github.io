# Coefficients for the 50th lambda value
ridge.mod$lambda[50]  # Corresponds to the 50th lambda value
coef(ridge.mod)[, 50]  # Coefficients at the 50th lambda value

# Calculate the L2 norm (Euclidean norm) of coefficients at lambda[50] (excluding the intercept)
# As these coefficients have been significantly shrunk, L2 regularisation has had a strong effect
sqrt(sum(coef(ridge.mod)[-1, 50]^2))  # L2 norm

# Coefficients for the 60th lambda value
ridge.mod$lambda[60]  # Corresponds to the 60th lambda value
coef(ridge.mod)[, 60]  # Coefficients at the 60th lambda value

# Calculate the L2 norm of coefficients at lambda[60] (excluding the intercept)
# As the results are larger compared to those at lambda[50], this indicates
# that L2 regularisation has penalised the coefficients less 
sqrt(sum(coef(ridge.mod)[-1, 60]^2))  # L2 norm

# Use predict with lambda value corresponding to s = 50th lambda
# Using the results where lambda = 50, generated smaller coefficients that were
# less prone to overfitting. 
predicted_coefficients <- predict(ridge.mod, s = 50, type = "coefficients")[1:11, ]
predicted_coefficients

# Train-test split
train = sample(1:nrow(x), nrow(x) / 2)
test = (-train)
y.test = y[test]

# Fit a Ridge regression model on the training set to evaluate the test MSE
# Using a large lambda value of 4
ridge.mod = glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred = predict(ridge.mod, s = 4, newx = x[test,])
mean((ridge.pred - y.test)^2)

# Method to calculate the test MSE using intercepts
mean((mean(y[train]) - y.test)^2)

# Fit a Ridge regression model with an extremely large lambda (1e10) and calculate the test MSE
ridge.pred = predict(ridge.mod, s = 1e10, newx = x[test,])
mean((ridge.pred - y.test)^2)

# Another method to calculate test MSE with exact regularisation and training data considered
ridge.pred = predict(ridge.mod, s = 1e10, newx = x[test,], exact = TRUE, x = x, y = y)
mean((ridge.pred - y.test)^2)

# Fitting a Ridge regression model where lambda = 4 leads to a much lower test MSE 
# than a model with just an intercept

# Fit a Ridge regression model without regularisation (s = 0) and calculate the test MSE
ridge.pred = predict(ridge.mod, s = 0, newx = x[test,], exact = TRUE, x = x, y = y)
mean((ridge.pred - y.test)^2)

# Fit an unpenalised least squares model for comparison
lm(y ~ x, subset = train)
predict(ridge.mod, s = 0, exact = TRUE, x = x, y = y, type = "coefficients")[1:11,]

# This indicates that a wide range of lambda values can be used.
# Very small lambda values (e.g., log(lambda) < -2) result in minimal regularisation,
# while larger lambda values penalise large coefficients.
# The optimal lambda lies between these extremes, minimising the test MSE 
# while shrinking the coefficients to balance model complexity.

# Cross-validation to determine the optimal lambda value
cv.out = cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)

# The best lambda value that results in the smallest MSE is determined by cross-validation
bestlam = cv.out$lambda.min
bestlam

# Calculate the optimal test MSE associated with the best lambda value
ridge.pred = predict(ridge.mod, s = bestlam, newx = x[test,])
mean((ridge.pred - y.test)^2)

# Refit the Ridge regression model on the full dataset using the optimal lambda 
# and examine the coefficient estimates
out = glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:11,]


# Fit Lasso regression model with cross-validation to determine the best lambda
cv.lasso <- cv.glmnet(x[train,], y[train], alpha = 1)

# Plot the cross-validation results
plot(cv.lasso)

# Extract the best lambda from cross-validation
bestlam <- cv.lasso$lambda.min

# Use the best lambda to predict on the test data
lasso.pred <- predict(cv.lasso, s = bestlam, newx = x[test,])

# Calculate the test MSE
mean((lasso.pred - y.test)^2)

# Refit the Lasso model on the entire dataset using the best lambda
final.lasso <- glmnet(x, y, alpha = 1, lambda = bestlam)

# Extract the final coefficients
lasso.coef <- predict(final.lasso, type = "coefficients")[1:11, ]
print(lasso.coef)

# Print non-zero coefficients
lasso.coef[lasso.coef != 0]
