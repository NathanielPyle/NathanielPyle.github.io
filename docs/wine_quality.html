<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Wine Quality Dataset</title>

<script src="site_libs/header-attrs-2.27/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/darkly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Nathaniel Pyle</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="project.html">Projects</a>
</li>
<li>
  <a href="aboutme.html">About Myself</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Wine Quality Dataset</h1>

</div>


<div id="overview" class="section level2">
<h2>Overview</h2>
<ul>
<li><strong>Description</strong>: The quality of wine is determined by
various features that relate to its taste, aroma, and other
characteristics familiar to wine experts. The UC Irvine Wine Dataset,
available at <a
href="https://archive.ics.uci.edu/dataset/186/wine+quality">UC Irvine
Wine Dataset</a>, c consists of two datasets: one for white wine and one
for red wine. These datasets evaluate wine quality using physicochemical
tests and sensory variables, focusing on the red and white variants of
Portuguese Vinho Verde wine.</li>
</ul>
<p>The dataset includes 11 features and one dependent variable, the wine
quality score. The features, which are based on physicochemical inputs
and sensory data, include: fixed acidity, volatile acidity, citric acid,
residual sugar, chlorides, free sulphur dioxide, total sulphur dioxide,
density, pH, sulphates, and alcohol. These measurements were taken based
on sensory evaluations, with a median score determined from at least
three wine expertsâ€™ assessments.</p>
<p>Each expert graded the wine samples on a scale from 0 (very bad) to
10 (excellent). Due to privacy and logistical constraints, only
physicochemical inputs and sensory outputs are available in this
dataset. No information about grape type, wine brand, or other
confounding variables is provided.</p>
<p>Notably, the dataset contains no missing data.</p>
<p>The aim of this project was to identify the key variables that
influence wine quality. To achieve this, various data science techniques
were applied, including correlation matrices, subset selection, model
evaluation, stepwise selection, validation and cross-validation using
K-folds, as well as Ridge and Lasso regression.</p>
</div>
<div id="methodology" class="section level2">
<h2>Methodology</h2>
<ul>
<li><p><strong>Technologies</strong>: the packages that were used in
this project were <code>corrplot</code>, <code>readODS</code>,
<code>ISLR</code>, <code>leaps</code>, and <code>glmnet</code>.</p></li>
<li><p><strong>Methods</strong>: The columns in both datasets were
assigned names from the source files and the accompanying README.
Initially, correlation matrices were created to uncover any basic
correlations between variables. To no avail, these correlation matrices
provided little information about which variables were the most
important, or the nature of the data itself. Therefore, more technical
techniques were required to analyse the data itself.</p></li>
</ul>
<p>To overcome the limitations of correlation matrices, subset selection
was used to analyse the coefficient of determination(R-Squared) across
models as additional variables were incrementally added. Plots were
created to show correlation on the y-axis, and the number of variables
on the x-axis.</p>
<p>These plots indicated that as more variables are added, the R-squared
value increased, meaning the model explains more variance. However,
there is a point of diminishing returns where adding additional
variables no longer significantly improves the modelâ€™s performance.
Including all variables in the model would increase accuracy but risk
overfitting. Both datasets exhibited this diminishing return effect.</p>
<p>Four plots were created to show the number of variables against
Correlation, Adjusted-Correlation, Mallowâ€™s Cp, and Bayesian Information
Criterion(BIC). An optimal point was identified for each plot, except
for the Correlation plot. In addition to these plots, regression subset
plots were generated to show which variables were the most important
with regards to the Correlation, Adjusted R-squared, Mallowâ€™s Cp and
BIC. These are similar to bar charts, and show that the higher the bars,
the more important the variable.</p>
<p>Next, Stepwise Selection modelsâ€”both Forward and Backwardâ€”were
applied to the White and Red wine datasets. These models aimed to
identify the variables that explained the variance in wine quality.
Stepwise Selection helps reduce overfitting and eliminate unnecessary
variables by iteratively adding and removing variables from the model.
Variables not included in the final model were deemed unimportant.</p>
<p>To validate the findings from the subset selection and Stepwise
Selection models, validation and cross-validation were performed. The
data for both datasets was split into training and testing partitions.
Subset selection was used on the training data to identify the best
variables for predicting wine quality, while the testing data was used
to construct a model matrix for predictions.</p>
<p>A loop was implemented to calculate the Mean Squared Error (MSE) for
models with varying numbers of predictors from 1 to 11. For each model,
coefficients were extracted and used to generate predictions by
multiplying them with the appropriate columns of the test model matrix.
These predictions were compared with the actual quality values in the
test dataset to calculate MSE, which were stored as validation
errors.</p>
<p>The validation errors were analysed to determine which model produced
the lowest test MSE. This allowed a comparison between these results,
and the previous findings from previous Subset Regression and Stepwise
Selection methods. If the number of variables is the same as the
previous results, with test MSE in consideration as well, this serves as
validation for those findings.</p>
<p>It is important to note that while this approach provides a set of
coefficient values for the best model, these values may differ slightly
from those produced by Stepwise Selection models. This variation is
expected due to the random splitting of the data into training and
testing sets. Consequently, while the coefficients will be similar, they
may not be identical.</p>
<p>To ease and facilitate predictions, a custom prediction function
â€˜predict.regsubsetsâ€™ was created. This function accepts the selected
model, constructs a model matrix using new data, and applies the
corresponding coefficients to generate predictions. The goal of this
function is to enable seamless prediction from the regsubsets output,
which is not directly available in base R.</p>
<p>Best subset selection was performed on the full white and red wine
datasets, considering models with up to 11 variables. The coefficients
for the model with the top 8 variables were extracted, allowing for
comparison with previous methods. This step ensures that the full
dataset was evaluated for model selection.</p>
<p>To further validate model selection, a k-fold cross-validation
process (k = 10) was employed. The dataset was randomly partitioned into
10 folds, with each fold serving as a test set once while the remaining
folds formed the training set. Predictions were made for each model size
(from 1 to 11 variables) using the predict.regsubsets function, and MSE
was calculated for each fold. Cross-validation errors were stored in a
matrix.</p>
<p>After the cross-validation process, the average MSE for each model
size was calculated. These values were plotted to determine the optimal
number of variables based on the lowest cross-validation error. The
results indicated that the model with 8 variables for the white wine and
7 for the red produced the lowest mean cross-validation error,
validating the previous subset selection findings. The final models,
using 8 variables for the white wine dataset and 7 variables for the red
wine dataset were confirmed to be the most effective.</p>
<p>In conclusion, Ridge and Lasso regression were explored as
alternative approaches to identify the most important variables. This
method involved creating a matrix (x variable) for both the white and
red wine datasets using all independent variables, while the response
variable (y) was the quality of the wine. A grid of lambda values was
defined to test during regularisation. By varying the alpha parameter
between 0 (Ridge) and 1 (Lasso), a sequence of lambda values from the
grid was applied to regularise the model.</p>
<p>Different lambda values, including 50, 60, and the optimal minimum,
were explored, and the corresponding coefficients were analysed. These
coefficients were used to make predictions on both datasets. The model
was fitted on the training data, and the test MSE was calculated for
different lambda values, following a similar split of training and
testing data as in Stepwise Selection. An unpenalised least squares
model was also used for comparison. Cross-validation determined the
optimal lambda value that minimised the test MSE, providing a robust
evaluation of the model. Ridge and Lasso regression effectively
regularised the variables, with unimportant variables shrunk towards
zero.</p>
<p>Lasso regression followed a similar process to Ridge regression, with
the key difference being the alpha value set to 1. The same approach was
applied to identify the optimal MSE, and through cross-validation, the
best set of coefficients that minimised the test MSE was determined. In
Lasso regression, many unimportant variables were fully regularised to
zero.</p>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<p>Subset Selection The results from the white wine dataset were that
the model does not significantly improve after the 7th variable. From
the first variable to the 8th, the correlation increased from 0.1897253
to 0.2817520. At the 8th variable, the model converges and starts to
plateu.</p>
<p>In comparison the results for the red wine dataset increase rapidly
from the 1st variable to the 2nd, from a correlation of 0.2267344
0.3170024. Interestingly, at the 7th variable the model converges and
begins to plateu. This difference between the white and red wine
datasets indicates that the red wine dataset is more parsimonious than
the white wine dataset. These results are crucial for further regression
specific algorithms later on in this project.</p>
<p>The Adjusted R-squared plot shows the variability in the response
variable. It is important to note that the model adjusts for the number
of predictor variables and penalises multicollinearity. The threshold
appears to be around 8 variables, after which any further increase in
Adjusted R-squared becomes minimal, indicating that additional variables
contribute little to improving the model.</p>
<p>The Mallowâ€™s Cp plot strikes a balance between the number of
variables and the variance. The lowest Cp value is reached with 8
variables, suggesting this is the optimal number. Beyond this point,
additional variables offer little benefit and can be discarded.</p>
<p>The Bayesian Information Criterion (BIC) plot further supports this
conclusion. BIC penalises models that are overly complex, favouring
simpler, more parsimonious ones. This plot also suggests that a model
with 8 variables achieves the most efficient balance between complexity
and performance.</p>
<p>For reference, the 8 variables for the white wine dataset are: fixed
acidity, volatile acidity, residual sugar, free sulphur dioxide,
density, pH, sulphates and alcohol. In comparison, the 7 variables for
the red wine datasets are: fixed acidity, volatile acidity, citric acid,
chlorides, pH, sulphates and alcohol. As these datasets have some
similarity, there is overlap between the same variables.</p>
<p>When applying this reasoning to the red wine dataset, similar
patterns emerge, though with some differences. The Adjusted R-squared
still points towards 8 variables, but Mallowâ€™s Cp and BIC suggest
slightly fewerâ€”7 and 6 variables. This indicates some similarity between
the datasets, while also highlighting that the red wine dataset requires
fewer variables for an optimal balance between bias, variance, and
complexity. Due to this difference, for the white wine dataset 8
variables were used for stepwise selection whereas 7 were used for the
red wine dataset; as an average of Mallowâ€™s Cp, BIC and the Adjusted
R-Squared values. The regression subset plots show which variables are
the most important in both the white and red wine datasets. It should be
noted that these results are synonymous with the previous subset plots,
they just indicate which variables are the most important features.</p>
<p>As the results for both the White and Red wine datasets were the same
for both Forward and Backwards Stepwise Selection, this indicates that
both models are stable. Using 8 variables for the White wine and 7
variables for the Red wine datasets means that these number of variables
are the most significant features to each modelâ€™s performance. These
results are consistent with the Adjusted R-Squared values, Mallowâ€™s Cp
and BIC from stepwise selection. In addition to this, the values having
the same values for both Forward and Backwards Stepwise Selection
indicates that there are no confounding multicollinearity issues. In the
beginning of this project, correlation matrices were created to show the
relationships variables had with both itself and others. As the results
are the same for both Stepwise Selection methods, this indicates that
the order of the variables were added or subtracted from the model did
not influence the model.</p>
<p>Using the for loop-6 for the red wine</p>
<p>Using the loop to select the model with the lowest test MSE, for both
the white and red wine datasets, there is some variation. For the white
wine dataset, the for loop selects a model with 8 variables, which is
consistent with the previous results, however for the red wine dataset
the for loop selects a model with 6 variables. This score of 6 variables
is consistent with the BIC number of variables from Stepwise Selection.
Despite being a slight difference of 1 variable, this is not a huge
difference and does provide both reliability to the previous findings.
It is important to note that 8 and 6 variables were selected as they
produce the lowest number of validation errors out of all 11 possible
combinations. In addition to this, ensuring that the set.seed function
is used is crucial as without this function, the results are
inconsistent.</p>
</div>
<div id="visualisations" class="section level2">
<h2>Visualisations</h2>
</div>
<div id="citations" class="section level2">
<h2>Citations</h2>
<p>As requested by the authors of this dataset, they requested this
citation be used.</p>
<p>P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling
wine preferences by data mining from physicochemical properties. In
Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.</p>
</div>
<div id="further-links" class="section level2">
<h2>Further links</h2>
<p>For further links please access the following url.</p>
<p><a href="http://dx.doi.org/10.1016/j.dss.2009.05.016">Modeling wine
preferences by data mining from physicochemical properties</a></p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
